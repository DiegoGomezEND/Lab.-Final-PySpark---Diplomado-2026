# **An√°lisis de la Din√°mica Contractual del Estado Colombiano usando Datos Abiertos de SECOP II**

**Desarrollado por:** Diego Alejandro Gomez Cortes y Victor Hugo D√≠az

### **Introducci√≥n**

SECOP II (Sistema Electr√≥nico para la Contrataci√≥n P√∫blica) es la principal plataforma de transparencia y seguimiento de la contrataci√≥n estatal en Colombia. A trav√©s de esta plataforma se registran los contratos p√∫blicos de entidades nacionales y territoriales, permitiendo el acceso abierto a informaci√≥n clave como valores contratados, tipos de contrato, entidades, proveedores y estados contractuales. El volumen, diversidad y naturaleza p√∫blica de estos datos convierten a SECOP II en una fuente estrat√©gica para el an√°lisis econ√≥mico, administrativo y de pol√≠tica p√∫blica.

El an√°lisis de los datos de SECOP II permite identificar patrones de contrataci√≥n, concentraci√≥n de recursos, comportamientos por territorio y modalidades contractuales, as√≠ como posibles riesgos o ineficiencias en el uso de los recursos p√∫blicos. Dado que la contrataci√≥n estatal representa una porci√≥n significativa del gasto p√∫blico, su estudio sistem√°tico aporta valor tanto para la toma de decisiones institucionales como para el control ciudadano y el desarrollo de modelos anal√≠ticos y predictivos.

Desde una perspectiva de datos, SECOP II plantea retos propios de los entornos Big Data: grandes vol√∫menes de informaci√≥n, esquemas complejos, datos heterog√©neos y actualizaciones constantes. Por esta raz√≥n, el uso de herramientas distribuidas como Apache Spark, Visual Studio Code, Java, JupyterLab y Docker resultan fundamentales para procesar, explorar y preparar estos datos de forma eficiente y escalable.

### **Objetivo del An√°lisis**

El objetivo de este trabajo es comprender el comportamiento de la contrataci√≥n p√∫blica registrada en SECOP II, explorando sus principales caracter√≠sticas estructurales y econ√≥micas mediante t√©cnicas de an√°lisis de datos a gran escala. A partir de esta exploraci√≥n, se busca sentar las bases para la construcci√≥n de an√°lisis avanzados y modelos de Machine Learning que permitan extraer conocimiento √∫til sobre la din√°mica contractual del Estado colombiano.

## Carga e ingesta de datos

En la fase de ingesta se consumieron datos abiertos de **SECOP II** a trav√©s de la API de **Datos Abiertos Colombia**, conservando registros con **fecha de firma** diligenciada. Posteriormente, la informaci√≥n se carg√≥ en un entorno distribuido con **Apache Spark** (orquestado en contenedores con **Docker**) para garantizar reproducibilidad y escalabilidad.

Ya en Spark, se inspeccion√≥ el esquema, se estandarizaron nombres de columnas y tipos de dato, y se seleccionaron variables relevantes para el an√°lisis y el modelado. Finalmente, el conjunto resultante se almacen√≥ en formato **Parquet**, optimizado para lectura eficiente y procesamiento distribuido en etapas posteriores.

**Evidencia**: `01_ingesta_resuelto.ipynb`


## **An√°lisis Exploratorio de Datos (EDA)**

En esta fase se realiz√≥ un an√°lisis exploratorio sobre los contratos electr√≥nicos de SECOP II con el fin de comprender la estructura del dataset, validar su calidad y caracterizar el comportamiento del valor contractual antes de pasar a la ingenier√≠a de variables y al modelado. El EDA se enfoc√≥ en distribuci√≥n del valor del contrato, concentraci√≥n territorial, tipo de contrato, estado contractual y comportamiento temporal.

Primero, se verific√≥ el periodo temporal efectivo a partir de la variable **fecha_de_firma** (filtrando registros con firma reportada). Los resultados muestran que la mayor parte de los contratos analizados se concentra hacia finales de 2025 y los primeros registros de 2026, lo que indica que el dataset representa procesos contractuales recientes dentro del periodo observado.

Luego, se calcularon estad√≠sticas descriptivas de la variable objetivo **valor_del_contrato**. Se evidenci√≥ una distribuci√≥n **altamente asim√©trica**: la mayor√≠a de contratos se ubica en rangos bajos y medios, mientras que una proporci√≥n menor corresponde a contratos de gran escala. Este patr√≥n justifica el uso posterior de transformaciones (por ejemplo, logaritmo) y m√©tricas robustas para evitar que los valores extremos dominen el ajuste del modelo.

A nivel territorial, se observ√≥ una concentraci√≥n marcada de contratos en el **Distrito Capital de Bogot√°**, seguido por **Antioquia** y **Valle del Cauca** dentro del top 10 por n√∫mero de contratos (ver Figura 1). Este resultado respalda la necesidad de an√°lisis regionales m√°s detallados y de controles por territorio en etapas posteriores.


<img width="1157" height="465" alt="image" src="https://github.com/user-attachments/assets/b0f57285-8e74-47c2-b495-0bf131b51c7b" />


Tambi√©n se explor√≥ la distribuci√≥n por **tipo de contrato**, donde la modalidad de **Prestaci√≥n de Servicios** domina el volumen de registros, seguida por otras modalidades administrativas y contratos de compra/suministro. En cuanto al **estado del contrato**, predominan contratos en ejecuci√≥n y contratos con modificaciones, consistente con procesos contractuales activos.

Finalmente, se aplic√≥ el m√©todo **IQR (Interquartile Range)** para detectar valores at√≠picos en **valor_del_contrato**, excluyendo previamente valores iguales o menores a cero. Se identific√≥ aproximadamente un **15%** de contratos como outliers superiores. Estos casos no se interpretan como errores, sino como contratos de alta cuant√≠a que impactan fuertemente la distribuci√≥n; por ello, se consideran al momento de definir transformaciones y criterios de escalamiento para modelado.

El an√°lisis temporal por a√±o y mes evidenci√≥ picos de contrataci√≥n hacia el cierre de 2025 y una disminuci√≥n al inicio de 2026, coherente con din√°micas administrativas y ciclos presupuestales.

**Evidencia:** 02_exploracion_eda_resuelto.ipynb

## Feature Engineering y Construcci√≥n de Pipelines

En esta fase se prepararon los datos de **SECOP II** para su uso en modelos de *Machine Learning* con **Spark ML**, transformando variables categ√≥ricas en representaciones num√©ricas y consolid√°ndolas en un vector de caracter√≠sticas reutilizable mediante **Pipelines**. El proceso se aplic√≥ sobre una muestra de **100.000 contratos**, garantizando consistencia, escalabilidad y reproducibilidad en un entorno distribuido.

### Selecci√≥n de variables

Se seleccionaron variables con relevancia contractual y capacidad explicativa para capturar diferencias territoriales, administrativas y operativas entre los contratos:

- **Variables categ√≥ricas (features)**: `departamento`, `tipo_de_contrato`, `estado_contrato`  
  Estas variables fueron codificadas con transformaciones est√°ndar de Spark ML (indexaci√≥n y codificaci√≥n *one-hot*), y posteriormente ensambladas en un √∫nico vector de entrada para los modelos.

- **Variable objetivo (label)**: `valor_del_contrato_num`  
  Esta variable representa el valor monetario del contrato y corresponde al **resultado a predecir**, por lo que **no se utiliza como feature** en el entrenamiento. Dado su comportamiento altamente asim√©trico observado en el EDA, se trabaj√≥ con una transformaci√≥n del objetivo (por ejemplo, `valor_del_contrato_log`) para estabilizar la escala durante el ajuste del modelo. En producci√≥n, las predicciones pueden reconvertirse a la escala original cuando se requiere interpretaci√≥n monetaria.


---

## Limpieza de datos

Se aplic√≥ una limpieza basada en la eliminaci√≥n de valores nulos (*dropna*) √∫nicamente en las variables utilizadas por el modelo, con el objetivo de asegurar consistencia en el entrenamiento y evitar fallas en el pipeline.

- Registros antes de limpieza: **100.000**
- Registros despu√©s de limpieza: **100.000**

En esta muestra, las variables seleccionadas no presentaron valores nulos, por lo que no fue necesario descartar registros ni aplicar imputaciones para continuar con el modelado.


---

### Codificaci√≥n de variables categ√≥ricas

Las variables categ√≥ricas fueron transformadas usando:

1. **StringIndexer**: conversi√≥n de texto a √≠ndices num√©ricos
2. **OneHotEncoder**: transformaci√≥n de √≠ndices a vectores binarios

**Categor√≠as detectadas**

| Variable            | Categor√≠as √∫nicas |
|---------------------|------------------|
| departamento         | 34               |
| tipo_de_contrato     | 17               |
| estado_contrato      | 6                |

**Resultado del OneHotEncoding**

- Total de features categ√≥ricas generadas: **57**
- Features num√©ricas originales: **1**

**Total de features esperadas:** **58**


### Construcci√≥n del vector de features

Las variables num√©ricas y las categ√≥ricas codificadas fueron combinadas mediante **VectorAssembler**, generando un √∫nico vector:

- `features_raw`

**Validaci√≥n del resultado**

- Dimensi√≥n esperada del vector: **58**
- Dimensi√≥n real del vector `features_raw`: **58**

Esto confirma que el proceso de codificaci√≥n y ensamblaje se ejecut√≥ correctamente.

---

### Construcci√≥n del Pipeline

El pipeline se construy√≥ respetando el orden l√≥gico de las transformaciones:

1. StringIndexer  
2. OneHotEncoder  
3. VectorAssembler  

**Resultado del pipeline**

- Total de stages: **7**
  - 3 StringIndexer
  - 3 OneHotEncoder
  - 1 VectorAssembler

El pipeline fue entrenado y aplicado exitosamente sobre el dataset limpio, garantizando reproducibilidad y consistencia en futuras ejecuciones.

### An√°lisis de varianza de features

Se realiz√≥ un an√°lisis de varianza sobre el vector `features_raw` para identificar las dimensiones con mayor dispersi√≥n. Para evitar consumo excesivo de memoria, el c√°lculo se hizo sobre una muestra de **1.000 registros**, convirtiendo el vector de Spark a una matriz NumPy.

**Matriz evaluada:** (1000, 58)

**Top 5 features con mayor varianza (por √≠ndice del vector):**
- Feature 0: varianza = **3.5130**
- Feature 52: varianza = **0.2456**
- Feature 53: varianza = **0.2424**
- Feature 1: varianza = **0.2128**
- Feature 2: varianza = **0.0908**

En la pr√°ctica, esto indica que unas pocas dimensiones concentran la mayor parte de la variabilidad, lo cual respalda aplicar **normalizaci√≥n** y continuar con **reducci√≥n de dimensionalidad (PCA)** en la siguiente fase.

**Evidencia:** 03_feature_engineering_resuelto.ipynb

## Transformaciones Avanzadas

En esta fase se aplicaron transformaciones adicionales sobre el dataset ya construido en *Feature Engineering*, con el objetivo de mejorar la calidad de las variables de entrada, corregir diferencias de escala y preparar el espacio de caracter√≠sticas para el entrenamiento de modelos.

El punto de partida fue `secop_features.parquet`, que contiene el vector `features_raw` con **58** dimensiones, compuesto por **57 features categ√≥ricas** (OneHotEncoding) y **1 feature num√©rica**.

### ¬øPor qu√© normalizar?

Al revisar los primeros registros de `features_raw`, se evidenci√≥ una disparidad clara en las escalas: la variable num√©rica (asociada al valor del contrato) puede estar en √≥rdenes de millones, mientras que las variables categ√≥ricas codificadas toman valores binarios (0/1).  
Ejemplo observado en `features_raw`: `[4.5588e+06, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...]`

Esta diferencia de magnitudes puede afectar el entrenamiento, ya que las variables con valores grandes tienden a dominar el proceso de aprendizaje y dificultan la convergencia o la interpretaci√≥n del modelo.

### Comparaci√≥n antes y despu√©s de StandardScaler

Para corregir el problema de escalas, se aplic√≥ **StandardScaler** sobre el vector `features_raw`, generando un nuevo vector normalizado denominado `features_scaled`. Con esto, las dimensiones quedan en una escala comparable, reduciendo el sesgo por magnitud y dejando el dataset listo para t√©cnicas posteriores como reducci√≥n de dimensionalidad (por ejemplo, PCA) y entrenamiento de modelos.


### Estad√≠sticas comparativas (muestra)

**Antes (`features_raw`):**
- Min: 0.00  
- Max: 24.27  
- Media: 0.34  
- Desviaci√≥n est√°ndar: 2.23  

**Despu√©s (`features_scaled`):**
- Min: 0.00  
- Max: 72.55  
- Media: 0.35  
- Desviaci√≥n est√°ndar: 1.57  

Estos valores corresponden a una muestra utilizada para comparar el comportamiento del vector antes y despu√©s del escalado. El escalado estandariza las magnitudes y deja el dataset en mejores condiciones para aplicar PCA y entrenar modelos de forma m√°s estable.

## Configuraci√≥n de PCA y selecci√≥n del n√∫mero de componentes

Esta etapa aplica **PCA (An√°lisis de Componentes Principales)** sobre el vector **`features_scaled`** para reducir dimensionalidad y dejar un espacio de features m√°s manejable antes del entrenamiento.

- **Dimensi√≥n original del vector**: **58**
- **N√∫mero de componentes seleccionados (k)**: **30**
- **Salida del PCA**: **`features_pca`** (vector denso con 30 componentes)

La elecci√≥n de **k = 30** se hizo probando valores y revisando la **varianza explicada acumulada**, buscando un equilibrio entre reducci√≥n y retenci√≥n de se√±al.

**Varianza explicada acumulada (seg√∫n el ajuste de PCA):**
- **Componente 10**: **23.47%**
- **Componente 20**: **41.33%**
- **Componente 30**: **58.84%**

En este dataset la varianza queda relativamente ‚Äúrepartida‚Äù entre componentes, algo com√∫n cuando hay muchas variables categ√≥ricas codificadas. Aunque no se alcanza un 80% de varianza explicada, **30 componentes** reducen la dimensionalidad de **58 ‚Üí 30** (reducci√≥n de **48.3%**) manteniendo una porci√≥n relevante de informaci√≥n para modelado.


## Pipeline completo de transformaciones

Finalmente, se integraron las transformaciones en un **pipeline** para asegurar un flujo reproducible y consistente, tanto en entrenamiento como en etapas posteriores.

**Orden aplicado:**
1. **StandardScaler**
2. **PCA**

Este orden es clave, porque **PCA** es sensible a la escala: si no se normaliza antes, las variables con magnitudes mayores dominan la descomposici√≥n.

Como resultado, se gener√≥ el dataset final listo para modelado (**`secop_ml_ready.parquet`**), con:
- **`features_pca`**: vector reducido de caracter√≠sticas (**k = 30**)
- **`label`**: variable objetivo del modelo (**`valor_del_contrato_log`**)

Adicionalmente, el pipeline entrenado se dej√≥ persistido para reutilizarlo en procesos futuros y mantener consistencia entre entrenamiento e inferencia.

---


## Modelos de Regresi√≥n

### Regresi√≥n lineal

Se construy√≥ un modelo de **Regresi√≥n Lineal** en Spark ML con el objetivo de predecir el valor del contrato a partir del vector de caracter√≠sticas generado en las fases anteriores (Feature Engineering + PCA). El dataset utilizado corresponde al archivo `secop_ml_ready.parquet`, el cual contiene las variables transformadas y listas para modelado.

#### Estrategia de Train/Test Split

Se defini√≥ una partici√≥n **70% entrenamiento / 30% prueba**, utilizando una semilla fija para garantizar reproducibilidad.

**Resultados del split:**
- **Train:** 70,126 registros (70%)
- **Test:** 29,874 registros (30%)

Esta proporci√≥n es adecuada para un dataset de 100.000 registros, ya que permite entrenar con suficiente informaci√≥n sin perder capacidad de evaluaci√≥n.


**Configuraci√≥n del Modelo de Regresi√≥n Lineal**

Se configur√≥ un modelo base de `LinearRegression` sin regularizaci√≥n, con los siguientes par√°metros:

- `featuresCol`: `features`
- `labelCol`: `label`
- `maxIter`: 100
- `regParam`: 0.0
- `elasticNetParam`: 0.0

Este modelo sirve como **baseline**, permitiendo evaluar el comportamiento inicial antes de aplicar regularizaci√≥n en fases posteriores.

### Interpretaci√≥n del R¬≤ del Modelo (Train)

El modelo se entren√≥ sobre el conjunto de entrenamiento, obteniendo las siguientes m√©tricas:

- **R¬≤ (train): 0.7417**
- **RMSE (train): 0.94**

Dado que la variable objetivo se trabaj√≥ en **escala logar√≠tmica** (`valor_del_contrato_log`), estas m√©tricas se interpretan en esa misma escala. En t√©rminos pr√°cticos, el R¬≤ sugiere que el modelo captura una parte importante de la variabilidad del **log del valor contractual** en entrenamiento, mientras que el RMSE indica el tama√±o t√≠pico del error (tambi√©n en log), lo que es coherente para un modelo lineal base en un problema con alta dispersi√≥n.

**An√°lisis de calidad de predicciones y errores**  
En el conjunto de prueba se revisaron las predicciones calculando:
- error absoluto  
- error porcentual  
- identificaci√≥n de las predicciones con mayor error


**Hallazgos clave:**

1. Se observaron errores absolutos elevados en contratos de valores muy altos.
2. Los errores porcentuales superan el 100% en contratos peque√±os, lo cual evidencia:
  - Alta dispersi√≥n en los valores
  - Dificultad del modelo lineal para capturar extremos
3. No se detectaron errores sistem√°ticos por duplicaci√≥n o fallas de c√°lculo.

Estos resultados son coherentes con la naturaleza altamente asim√©trica del valor de los contratos.

### Comparaci√≥n Train vs Test (Overfitting)

Se evalu√≥ el modelo sobre el conjunto de prueba:

**M√©tricas en Test:**
- **RMSE:** 0.9551  
- **MAE:** 0.4624  
- **R¬≤ (test): 0.7440**

**Comparaci√≥n Train vs Test:**
- **R¬≤ Train:** 0.7417  
- **R¬≤ Test:** 0.7440  
- **Diferencia absoluta:** 0.0023  

No se evidencia **overfitting**, ya que el desempe√±o en test es pr√°cticamente igual al de train. El modelo generaliza de forma consistente dentro de lo esperable para una regresi√≥n lineal base.

## An√°lisis de coeficientes del modelo

- **Intercepto:** 8.79  
- **N√∫mero de coeficientes:** 30 (corresponden a los **30 componentes PCA**, no a variables originales)

El intercepto representa el valor base estimado de la **variable objetivo en escala logar√≠tmica** (`label = valor_del_contrato_log`) cuando el vector de componentes PCA toma valores cercanos a cero.  
Dado que las *features* provienen de **PCA**, los coeficientes no se interpretan como ‚Äúefecto de una variable‚Äù espec√≠fica, sino como la contribuci√≥n de cada **componente principal** (combinaciones de las variables originales).

## An√°lisis de distribuci√≥n de residuos

Se calcul√≥ el residuo como:

**residuo = label ‚àí prediction**

El histograma muestra que los residuos est√°n **concentrados alrededor de 0**, lo que indica que el modelo captura razonablemente el comportamiento central. Tambi√©n se observan **colas y algunos valores extremos**, consistentes con la alta dispersi√≥n de los contratos: los errores tienden a aumentar cuando el contrato es at√≠pico (muy alto o muy bajo) incluso trabajando en escala logar√≠tmica.


**Resultado visual:**

<img width="993" height="468" alt="image" src="https://github.com/user-attachments/assets/b7c78c19-cf7f-4e7e-8815-5346d0d7cb63" />



## Feature importance aproximado (componentes PCA)

Para identificar qu√© componentes influyen m√°s, se ordenaron los coeficientes por **valor absoluto**.  
**Top 10 componentes m√°s influyentes (|coef|):**

1. **PC3**  (coef = -1.225635)
2. **PC4**  (coef =  0.290171)
3. **PC1**  (coef = -0.118095)
4. **PC8**  (coef = -0.114240)
5. **PC5**  (coef = -0.087070)
6. **PC6**  (coef =  0.076073)
7. **PC17** (coef = -0.056776)
8. **PC21** (coef = -0.055518)
9. **PC30** (coef = -0.054747)
10. **PC28** (coef =  0.051529)

Estos resultados indican que **unas pocas componentes concentran la mayor parte del efecto lineal**, mientras que el resto aporta ajustes marginales. Al ser PCA, esto se entiende como influencia de **combinaciones lineales** de las variables originales, no de una variable individual.

--- 


## Regresi√≥n Log√≠stica para Clasificaci√≥n de Riesgo Contractual

En esta fase se entren√≥ un modelo de **regresi√≥n log√≠stica** con el objetivo de **clasificar contratos del SECOP II** seg√∫n su nivel de riesgo, a partir de las variables preparadas en las etapas previas del pipeline. A diferencia de la regresi√≥n lineal (enfoque de predicci√≥n continua), aqu√≠ se plantea un problema de **clasificaci√≥n binaria**, donde el inter√©s es estimar la probabilidad de que un contrato sea **alto riesgo**.

El dataset utilizado corresponde a `secop_features.parquet`, con **100.000** contratos y variables categ√≥ricas codificadas junto con variables num√©ricas listas para modelado.

**Notebook/Query:** `06_regresion_logistica.py`

### Creaci√≥n de la variable objetivo binaria (`riesgo`)

Como el dataset original no incluye una variable expl√≠cita de riesgo, se construy√≥ una variable objetivo binaria denominada `riesgo`, combinando dos criterios:

- **Riesgo financiero:** contratos cuyo **valor** supera el **percentil 90** del valor del contrato.
- **Riesgo operativo:** contratos con **duraci√≥n definida** menor a **30 d√≠as** (cuando este dato est√° disponible).

El criterio financiero se aplic√≥ a todos los contratos. El criterio operativo se aplic√≥ √∫nicamente cuando la duraci√≥n estaba disponible; cuando no lo estaba, el contrato se clasific√≥ solo con base en el criterio financiero para evitar excluir registros del an√°lisis.

**Distribuci√≥n final de la variable `riesgo`:**
- **Alto riesgo (1): 11.921 (11.9%)**
- **Bajo riesgo (0): 88.079 (88.1%)**
- **Total registros: 100.000**

Esta distribuci√≥n refleja una **clase positiva minoritaria** (alto riesgo), por lo que en la evaluaci√≥n del modelo se debe revisar el desempe√±o espec√≠ficamente sobre esa clase y no depender √∫nicamente de m√©tricas globales.

### An√°lisis del balance de clases

Se analiz√≥ la distribuci√≥n de la variable `riesgo` para evaluar el balance de clases antes de entrenar la regresi√≥n log√≠stica. Con base en el conteo del dataset, **88.079 contratos (88.1%)** corresponden a **bajo riesgo (0)** y **11.921 (11.9%)** a **alto riesgo (1)**, sobre un total de **100.000** registros.

Este resultado indica un desbalance moderado hacia la clase negativa. En la pr√°ctica, el modelo puede entrenarse como l√≠nea base, pero la evaluaci√≥n debe centrarse en m√©tricas que reflejen el desempe√±o sobre la clase positiva (alto riesgo) y, si el objetivo es priorizar detecci√≥n, puede requerirse ajuste del umbral de decisi√≥n o t√©cnicas de balanceo en etapas posteriores.

### Diferencia entre regresi√≥n log√≠stica y regresi√≥n lineal

La regresi√≥n log√≠stica se usa para **clasificaci√≥n**, no para predicci√≥n de valores continuos. Estima la probabilidad de pertenecer a una clase (por ejemplo, `riesgo = 1`) aplicando una funci√≥n sigmoide a la combinaci√≥n lineal de las variables de entrada, produciendo un valor entre 0 y 1.

En este trabajo, la regresi√≥n lineal se utiliz√≥ para estimar el **valor del contrato**, mientras que la regresi√≥n log√≠stica se emple√≥ para clasificar contratos seg√∫n su **nivel de riesgo**, coherente con la naturaleza de cada objetivo.


**Configuraci√≥n del modelo**

El modelo de regresi√≥n log√≠stica se configur√≥ con los siguientes par√°metros:

- `maxIter = 100`
- `regParam = 0.0`
- `threshold = 0.5`

Esta configuraci√≥n corresponde a un modelo base sin regularizaci√≥n, utilizado como punto de referencia antes de introducir penalizaciones y ajustes m√°s avanzados. El threshold de 0.5 se utiliz√≥ inicialmente como valor est√°ndar, siendo posteriormente analizado y ajustado en los retos bonus.

**Interpretaci√≥n de probabilidades**

El modelo genera una columna `probability` con la forma:

- `probability = [p(clase=0), p(clase=1)]`

Es decir, el **segundo valor** corresponde a la probabilidad de **alto riesgo (clase 1)**.  
Ejemplo: si `probability = [0.80, 0.20]`, entonces `p(alto_riesgo)=0.20` y, con `threshold=0.5`, el contrato se clasifica como **bajo riesgo (0)**.

**Evaluaci√≥n del modelo con m√∫ltiples m√©tricas**

Para evaluar el desempe√±o del modelo se utilizaron m√©tricas apropiadas para clasificaci√≥n binaria:

- **AUC-ROC:** `0.9431`
- **Accuracy:** `0.9652`
- **Precision (weighted):** `0.9652`
- **Recall (weighted):** `0.9652`
- **F1-Score (weighted):** `0.9630`

El valor de AUC-ROC indica una buena capacidad del modelo para discriminar entre contratos de alto y bajo riesgo, independientemente del threshold utilizado. Las m√©tricas de precision y recall muestran un desempe√±o balanceado, lo que sugiere que el modelo logra identificar contratos riesgosos sin generar un n√∫mero excesivo de falsas alarmas.

**Matriz de confusi√≥n**

Se construy√≥ la matriz de confusi√≥n sobre el conjunto de prueba, obteniendo:

- **True Positives (TP)**: 2.587  
- **True Negatives (TN)**: 26.204  
- **False Positives (FP)**: 89  
- **False Negatives (FN)**: 950  
- **Total evaluado**: 29.830  

En este caso, el error m√°s cr√≠tico son los **falsos negativos (FN)**, porque representan contratos de **alto riesgo** clasificados como **bajo riesgo**. Con estos resultados, el modelo mantiene una **precisi√≥n alta** cuando predice ‚Äúalto riesgo‚Äù (pocos FP), pero deja escapar una parte relevante de casos realmente riesgosos (FN).

**Ajuste del threshold de clasificaci√≥n**

Se evalu√≥ el desempe√±o del modelo utilizando diferentes valores de threshold. Al aumentar el threshold a 0.7, se obtuvo:

- Accuracy: **0.950**
- Recall: **0.950**

Este resultado confirma el trade-off entre precisi√≥n y sensibilidad. Un threshold m√°s alto hace al modelo m√°s conservador, reduciendo la detecci√≥n de contratos riesgosos. Dado el objetivo del an√°lisis, se concluye que no es conveniente utilizar un threshold elevado y que valores cercanos o inferiores a 0.5 son m√°s adecuados para priorizar la detecci√≥n de riesgo.

**Curva ROC**

Se construy√≥ la curva ROC para visualizar el trade-off entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) a lo largo de distintos thresholds. La curva se encuentra claramente por encima de la l√≠nea de clasificaci√≥n aleatoria, confirmando la capacidad discriminatoria del modelo.

<img width="736" height="505" alt="image" src="https://github.com/user-attachments/assets/6782ae8d-db55-4461-bffb-6874a5759a04" />


La curva ROC muestra que el modelo discrimina muy bien entre contratos de **alto** y **bajo riesgo**. El **AUC = 0.943** indica un desempe√±o claramente superior al azar (0.5) y cercano a un clasificador excelente: el modelo tiende a asignar probabilidades m√°s altas a los contratos de alto riesgo. Al estar la curva por encima de la diagonal, es posible lograr **alta detecci√≥n (TPR)** con un **nivel moderado de falsas alarmas (FPR)**, ajustando el *threshold* seg√∫n el objetivo (por ejemplo, priorizar *recall* si es m√°s costoso dejar pasar contratos riesgosos).


## Regularizaci√≥n L1, L2 y ElasticNet

En esta fase se evalu√≥ el efecto de la regularizaci√≥n sobre el modelo de regresi√≥n lineal entrenado con **componentes PCA**. La intenci√≥n fue comprobar si introducir penalizaci√≥n **L2 (Ridge)**, **L1 (Lasso)** o una combinaci√≥n **ElasticNet** mejora la estabilidad del modelo y su desempe√±o en datos no vistos, manteniendo el mismo pipeline de variables ya transformadas.

**Dataset utilizado:** `secop_ml_ready.parquet`  
**Variables para modelado:** `features = features_pca` y `label = valor_del_contrato_log` (escala log).  
**Partici√≥n:** 70% entrenamiento / 30% prueba (**Train: 70,170** | **Test: 29,830**, semilla 42).

### Motivaci√≥n (por qu√© regularizar)
Aunque el baseline ya permite una primera aproximaci√≥n, la regularizaci√≥n ayuda a **controlar la magnitud de los coeficientes** y a reducir sensibilidad a variaciones del dataset. En un escenario con variables transformadas (PCA), esto se traduce en un modelo potencialmente m√°s **robusto** y con mejor capacidad de generalizaci√≥n al comparar m√©tricas en test.

### Configuraci√≥n de evaluaci√≥n
La comparaci√≥n entre configuraciones se hizo con **RMSE** como m√©trica principal, calculado sobre el conjunto de prueba. Dado que la variable objetivo est√° en **log**, el RMSE se interpreta en esa escala (no en pesos), por lo que sirve para comparar modelos de forma consistente dentro del mismo enfoque.

**M√©trica utilizada:** `rmse`


**Experimento con M√∫ltiples Regularizaciones**

Se entrenaron 18 modelos variando:

**regParam (Œª):**

- 0.0  
- 1.0  
- 10.0  
- 100.0  
- 1000.0  
- 10000.0  

**elasticNetParam (Œ±):**

- 0.0 ‚Üí Ridge  
- 0.5 ‚Üí ElasticNet  
- 1.0 ‚Üí Lasso  

Total modelos entrenados: 18  

**Resultados del Experimento**

Modelo sin regularizaci√≥n (Œª = 0.0):
- RMSE Train: 0.94
- RMSE Test: 0.96

Ridge (Œ± = 0.0, Œª = 1.0):
- RMSE Test: 1.12

ElasticNet (Œ± = 0.5, Œª = 1.0):
- RMSE Test: 1.29

Lasso (Œ± = 1.0, Œª = 1.0):
- RMSE Test: 1.47

A medida que Œª aumenta, el RMSE en test tambi√©n aumenta, lo que sugiere p√©rdida de capacidad predictiva (underfitting). El menor RMSE se obtuvo con Œª = 0.0; en este caso, la regularizaci√≥n no mejor√≥ el desempe√±o del modelo.

**Comparaci√≥n de Overfitting**

Brecha aproximada:

Gap = RMSE_Test ‚àí RMSE_Train
Gap ‚âà 947,902,345.85

La brecha se mantiene pr√°cticamente constante en todas las configuraciones.

Interpretaci√≥n:

- No se observa reducci√≥n significativa del overfitting mediante regularizaci√≥n.
- El modelo basado en PCA ya presenta estabilidad estructural.
- Incrementar Œª solo aumenta el error sin reducir la brecha train-test.

**Modelo Final**

Se seleccion√≥ como mejor modelo:

- regParam = 0.0  
- elasticNetParam = 0.0  

Resultados finales:

- RMSE Train: 0.94
- RMSE Test: 0.96

Modelo guardado en: /opt/spark-data/raw/regularized_model

**Efecto de Œª en Lasso**

Se evalu√≥ c√≥mo cambia la **sparsity** (cantidad de coeficientes llevados a cero) al aumentar **Œª (regParam)** en **Lasso (L1)**, usando un modelo entrenado sobre **30 componentes PCA**.

**Resultados (coeficientes en 0 / 30 | RMSE test):**
- Œª = 0.01 ‚Üí 8/30 coeficientes en 0 | RMSE test: 0.96  
- Œª = 0.10 ‚Üí 25/30 coeficientes en 0 | RMSE test: 1.00  
- Œª = 1.00 ‚Üí 29/30 coeficientes en 0 | RMSE test: 1.47  
- Œª = 10.00 ‚Üí 30/30 coeficientes en 0 | RMSE test: 1.88  
- Œª = 100.00 ‚Üí 30/30 coeficientes en 0 | RMSE test: 1.88  
- Œª = 1000.00 ‚Üí 30/30 coeficientes en 0 | RMSE test: 1.88  

A medida que **Œª aumenta**, Lasso **fuerza m√°s coeficientes a cero** (termina anulando el modelo cuando Œª ‚â• 10, dejando **30/30** en cero). En paralelo, el **RMSE en test empeora**, por lo que en este experimento la regularizaci√≥n fuerte no aporta y conviene mantener **Œª bajo** si se busca conservar capacidad predictiva.

#### Visualizaci√≥n del efecto de Œª

Efecto de Œª en la sparsity (Lasso):

<img width="826" height="514" alt="image" src="https://github.com/user-attachments/assets/3c9faffa-9530-44f1-860f-18e79a731c60" />



Efecto de Œª en el error del modelo:

<img width="598" height="346" alt="image" src="https://github.com/user-attachments/assets/41e0faa3-739e-4190-8dc1-b511c2e71dc2" />


Las dos gr√°ficas muestran un patr√≥n coherente: a medida que aumenta **Œª (regParam)** en Lasso, el modelo aplica una penalizaci√≥n m√°s fuerte, lo que se refleja en **m√°s coeficientes llevados a cero** y, al mismo tiempo, en un **cambio progresivo del RMSE en test** hasta llegar a una zona donde el comportamiento se vuelve **estable**. En conjunto, esto confirma que **Œª controla directamente el nivel de simplicidad del modelo** (sparsity) y permite observar con claridad el **trade-off** entre regularizaci√≥n y desempe√±o, facilitando la selecci√≥n de un valor de Œª acorde al objetivo del an√°lisis.

## **Optimizacion de Hiperparametros**

### **Validacion Cruzada**

En esta fase se aplic√≥ **validaci√≥n cruzada K-Fold** para obtener una estimaci√≥n m√°s robusta del desempe√±o del modelo, reduciendo la dependencia de una sola partici√≥n train/test. El flujo consiste en dividir el conjunto de entrenamiento en **K folds** y entrenar el modelo K veces, usando en cada iteraci√≥n **K‚àí1 folds para entrenar** y **1 fold para validar**, promediando la m√©trica para estabilizar la evaluaci√≥n. El dataset utilizado fue `secop_ml_ready.parquet`, con partici√≥n **80% entrenamiento / 20% prueba** (semilla 42), obteniendo **Train: 80,068** y **Test: 19,932** registros.

Para la b√∫squeda se construy√≥ un **ParamGrid** con **36 combinaciones**, variando `regParam` (0.01, 0.1, 1.0), `elasticNetParam` (0.0, 0.5, 1.0), `maxIter` (50, 100) y `fitIntercept` (True, False). Con **K=5 folds**, esto implic√≥ entrenar **180 modelos** en total (36 √ó 5), evaluando cada configuraci√≥n con **RMSE** como m√©trica principal.

El mejor modelo seleccionado por Cross-Validation correspondi√≥ a una configuraci√≥n tipo **Ridge** (`elasticNetParam = 0.0`) con regularizaci√≥n suave (`regParam = 0.01`). En los resultados del notebook, el **mejor RMSE en validaci√≥n cruzada fue 0.95**, y al evaluar esa configuraci√≥n sobre el conjunto de prueba se obtuvo un **RMSE en test de 0.94**, lo que indica un desempe√±o consistente fuera de muestra (teniendo en cuenta que el objetivo est√° en escala logar√≠tmica).

Adicionalmente, se prob√≥ el efecto del n√∫mero de folds con **K = 3, 5 y 10**. El **mejor RMSE se mantuvo estable (0.95)** en los tres casos, mientras que el **tiempo total** aument√≥ con K (**21.7s**, **35.3s** y **68.6s**, respectivamente), reflejando el trade-off esperado entre robustez de validaci√≥n y costo computacional.

## Optimizaci√≥n de Hiperpar√°metros

Se optimizaron hiperpar√°metros del modelo de **Regresi√≥n Lineal con regularizaci√≥n** usando el dataset `secop_ml_ready.parquet`, donde la variable objetivo corresponde a `valor_del_contrato_log` (por eso el **RMSE se interpreta en la escala log** del label). Para garantizar reproducibilidad, se mantuvo un **split 80/20** con **semilla 42**, obteniendo **80,068 registros en train** y **19,932 en test**.

Se ejecut√≥ un **Grid Search con Validaci√≥n Cruzada (K=3)** explorando combinaciones de `regParam` (0.01, 0.1, 1.0), `elasticNetParam` (0.0, 0.5, 1.0) y variaciones de configuraci√≥n del estimador. El mejor modelo encontrado fue **regParam = 0.01**, **elasticNetParam = 0.0 (Ridge/L2)**, **maxIter = 50** y **fitIntercept = True**, con un **RMSE promedio en Cross-Validation ‚âà 0.94** y un **RMSE en test ‚âà 0.94**. El tiempo total de ejecuci√≥n de esta b√∫squeda fue de **27.07 s**.

En paralelo, se aplic√≥ **TrainValidationSplit** con **trainRatio = 0.8** para comparar desempe√±o y costo computacional. Esta estrategia encontr√≥ exactamente la misma configuraci√≥n √≥ptima (**regParam = 0.01**, **elasticNetParam = 0.0**, **maxIter = 50**, **fitIntercept = True**) y obtuvo el mismo **RMSE en test ‚âà 0.94**, con un tiempo de ejecuci√≥n menor de **8.03 s**, lo que sugiere consistencia del √≥ptimo bajo ambos enfoques.

Finalmente, se realiz√≥ un refinamiento local alrededor de la mejor regi√≥n (incluyendo valores cercanos como **regParam = 0.005, 0.01, 0.02** y `elasticNetParam` en **0.0, 0.25, 0.5**), sin observar mejoras sobre el **RMSE ‚âà 0.94**. Con base en esto, se seleccion√≥ como estrategia final **Grid Search + CV** y se guard√≥ el modelo √≥ptimo en `/opt/spark-data/raw/tuned_model`, junto con los hiperpar√°metros en `/opt/spark-data/raw/hiperparametros_optimos.json`.


# **MLOps y Produccion**

## **MLflow Tracking**

En este reto se configur√≥ la conexi√≥n con el servidor de MLflow utilizando la URI `http://mlflow:5000`, lo que permiti√≥ centralizar el registro de experimentos en un servidor dedicado y no en archivos locales. Se cre√≥ el experimento `secop_prediccion`, donde se almacenan todos los runs relacionados con el modelo de predicci√≥n de contratos. Esto garantiza trazabilidad, organizaci√≥n y comparabilidad entre diferentes entrenamientos del modelo.

**Registro del Modelo Baseline**

Se entren√≥ un modelo base de regresi√≥n lineal sin regularizaci√≥n (`regParam=0.0`, `elasticNetParam=0.0`) utilizando como variable objetivo el **logaritmo del valor del contrato**, con el fin de estabilizar la varianza y reducir el impacto de valores extremos.

Las m√©tricas obtenidas fueron:

- **RMSE:** 0.89  
- **MAE:** 0.46  
- **R¬≤:** 0.73  

Estas m√©tricas est√°n calculadas en **escala logar√≠tmica**, lo que implica que los errores representan diferencias relativas y no absolutas en pesos colombianos. El modelo fue almacenado como artefacto dentro del run, permitiendo su posterior consulta o descarga desde la interfaz de MLflow.

**Registro de M√∫ltiples Modelos (Ridge, Lasso y ElasticNet)**

Se entrenaron tres modelos adicionales con diferentes tipos de regularizaci√≥n: Ridge (L2), Lasso (L1) y ElasticNet (L1 + L2), todos con `regParam=0.1`. Cada run registr√≥ par√°metros, m√©tricas (RMSE, MAE y R¬≤) y el modelo entrenado como artefacto.

üîπ **Ridge**
- **RMSE:** 0.89  
- **MAE:** 0.46  
- **R¬≤:** 0.7385  

üîπ**Lasso**
- **RMSE:** 0.92  
- **MAE:** 0.50 
- **R¬≤:** 0.7209 

 üîπ **ElasticNet**
- **RMSE:** 0.91 
- **MAE:** 0.48 
- **R¬≤:** 0.7312 

Se observ√≥ que el modelo Ridge obtuvo el mejor desempe√±o general en t√©rminos de RMSE y R¬≤, aunque las diferencias entre modelos fueron moderadas.


**Exploraci√≥n y Comparaci√≥n en MLflow UI**

Se utiliz√≥ la interfaz web de MLflow para comparar los runs lado a lado, ordenar por RMSE y analizar las diferencias en par√°metros y m√©tricas. Se observ√≥ que el modelo Ridge present√≥ el menor RMSE (‚âà 1.45), consolid√°ndose como la mejor alternativa dentro de los experimentos evaluados.

<img width="1600" height="596" alt="image" src="https://github.com/user-attachments/assets/8c50b8b1-cc03-4f86-a5e4-b0e986b90a3f" />



## **Registro de Artefactos Personalizados**

Se cre√≥ un nuevo run donde, adem√°s de registrar el modelo y m√©tricas, se agreg√≥ un reporte en texto con los resultados y un gr√°fico de **predicciones vs valores reales en escala logar√≠tmica**. Este gr√°fico permiti√≥ visualizar el comportamiento del modelo respecto a la l√≠nea de predicci√≥n perfecta y evidenciar la tendencia del modelo a concentrarse alrededor de la media.

Estos archivos fueron almacenados como artefactos dentro de MLflow, permitiendo su visualizaci√≥n directa desde la interfaz.

<img width="558" height="553" alt="image" src="https://github.com/user-attachments/assets/72253475-8111-4c72-ba0b-c8e344b54eb9" />



## **Model Registry**

El ciclo completo de gesti√≥n de modelos utilizando el **Model Registry de MLflow**, con el objetivo de versionar, promover y consumir modelos de forma controlada. Se logro, configurando la conexi√≥n al servidor de MLflow y se defini√≥ el nombre del modelo como `secop_prediccion_contratos` y se estableci√≥ el entorno para registrar versiones oficiales del modelo de predicci√≥n del valor de contratos SECOP.

En el primer paso se entren√≥ y registr√≥ la versi√≥n 1 (baseline) sin regularizaci√≥n, almacenando m√©tricas y registr√°ndola directamente en el Registry. Luego se entren√≥ la versi√≥n 2 con regularizaci√≥n (ElasticNet), comparando su RMSE frente al baseline. Tras la evaluaci√≥n, se determin√≥ que la versi√≥n 1 present√≥ un mejor desempe√±o, por lo que fue promovida a **Production**, mientras que la versi√≥n 2 fue archivada. Este proceso permiti√≥ aplicar correctamente el ciclo de vida: `None ‚Üí Staging ‚Üí Production ‚Üí Archived`.  
 
<img width="1600" height="439" alt="image" src="https://github.com/user-attachments/assets/dae1fcbe-e23c-4eb4-b9fb-056a31908170" />


Posteriormente se agreg√≥ metadata y descripci√≥n formal al modelo en producci√≥n, incluyendo versi√≥n, RMSE validado, dataset utilizado, tipo de features (PCA), autor y fecha. Esto garantiza trazabilidad, documentaci√≥n t√©cnica y gobernanza del modelo dentro del Registry.  
 
<img width="738" height="803" alt="image" src="https://github.com/user-attachments/assets/7270abf3-fe8c-431d-8986-919ef1874b6b" />


Finalmente, se carg√≥ el modelo directamente desde el Registry utilizando la URI l√≥gica: "models:/secop_prediccion_contratos/Production", se verific√≥ su correcto funcionamiento realizando predicciones sobre el conjunto de prueba. El RMSE obtenido coincidi√≥ exactamente con el registrado durante el entrenamiento, confirmando reproducibilidad, control de versiones y correcta gesti√≥n del ciclo de vida del modelo.

---

## Inferencia en Producci√≥n

En esta secci√≥n se simul√≥ un flujo t√≠pico de **MLOps** para generar **predicciones batch** en un entorno controlado. El objetivo es representar c√≥mo se ejecutar√≠a el modelo en producci√≥n: cargar un modelo versionado desde **MLflow Model Registry**, recibir datos nuevos, aplicar el modelo a gran escala y dejar resultados listos para consumo por otros equipos o sistemas. Este enfoque permite trazabilidad (qu√© modelo produjo qu√© predicci√≥n), repetibilidad y control de versiones, elementos clave cuando el modelo evoluciona en el tiempo.

La inferencia se plante√≥ como **Batch Inference**, ya que el caso consiste en correr predicciones sobre un volumen grande de contratos. A diferencia del entrenamiento, aqu√≠ el foco no est√° en ajustar par√°metros sino en **operacionalizar el modelo**: cargarlo desde `models:/{nombre_modelo}/Production`, lo cual es preferible a usar una ruta local porque desacopla el despliegue del entorno, facilita rollback (volver a una versi√≥n previa) y estandariza el ‚Äúmodelo oficial‚Äù que est√° activo en producci√≥n. Si no existe un modelo en el stage *Production*, el sistema deber√≠a manejarlo con control de errores y fallback (por ejemplo, cargar *Staging* o abortar el proceso dejando un log de alerta).

Para simular datos nuevos, se carg√≥ el parquet procesado y se prepar√≥ el esquema para que coincida con lo que el modelo espera, eliminando la columna objetivo (label). En un escenario real, estos datos podr√≠an llegar desde una base de datos transaccional, archivos en un data lake (S3/HDFS), una API, o incluso un stream; en todos los casos, la idea es que el modelo reciba √∫nicamente las **features** requeridas y que el pipeline valide el esquema antes de ejecutar predicciones.

Las predicciones se generaron usando `model.transform()` y se agreg√≥ un `prediction_timestamp`. Este timestamp es importante para trazabilidad, auditor√≠a y monitoreo, ya que permite reconstruir cu√°ndo se produjo cada predicci√≥n y asociarla con la versi√≥n del modelo y el lote de datos. Adem√°s del timestamp, en producci√≥n suele ser √∫til guardar metadatos como `model_version`, `batch_id`, `source_system`, `data_date` y un identificador √∫nico del contrato.

Luego se incluy√≥ un bloque de **monitoreo b√°sico** para verificar que las predicciones sean razonables: estad√≠sticas descriptivas (m√≠nimo, m√°ximo, promedio, desviaci√≥n est√°ndar), detecci√≥n de valores negativos si el contexto no los permite, y conteos por rangos para detectar colas extremas. Este monitoreo es una primera capa para identificar anomal√≠as y es el punto de partida para detectar **data drift**, comparando la distribuci√≥n actual de features y/o predicciones contra la distribuci√≥n hist√≥rica del entrenamiento. Si las predicciones empiezan a desviarse de lo esperado, lo correcto ser√≠a activar alertas, revisar la calidad de datos, y evaluar reentrenamiento o ajuste de umbrales seg√∫n el caso.

Finalmente, se guardaron los resultados en formatos consumibles. El formato **Parquet** es el m√°s adecuado para anal√≠tica interna y consumo en Spark por eficiencia y compresi√≥n; mientras que **CSV** es √∫til para consumo externo, validaciones r√°pidas o reportes que se abren en herramientas como Excel (aunque es menos eficiente). Para un dashboard interno normalmente se prioriza Parquet o una tabla en un motor anal√≠tico; para un reporte gerencial suele ser CSV o una tabla resumida; y para input de otro sistema suele preferirse Parquet o una tabla con esquema fijo (dependiendo del integrador).

En cuanto a automatizaci√≥n, este flujo se puede programar de forma peri√≥dica (por ejemplo diario o bajo demanda seg√∫n la llegada de lotes). Un orquestador como Airflow permite calendarizaci√≥n, dependencias, reintentos, logging y alertas. El monitoreo deber√≠a incluir comparaci√≥n de m√©tricas en el tiempo, chequeos de drift y umbrales de alarma. El reentrenamiento se dispara cuando el desempe√±o cae, el drift es persistente o cambian reglas de negocio. Las alertas t√≠picas se basan en cambios abruptos de distribuci√≥n, aumento de valores extremos, incremento sostenido de error (si se cuenta con verdad terreno posterior), o fallas de esquema/datos.

En resumen, esta secci√≥n replica el ciclo m√≠nimo de inferencia en producci√≥n: **cargar modelo versionado**, **preparar datos nuevos**, **predecir a escala**, **monitorear resultados**, **persistir outputs**, y **dise√±ar la automatizaci√≥n** para operaci√≥n continua.


---
## Conclusiones Finales

A lo largo del taller se construy√≥ un flujo completo y reproducible para analizar y modelar contratos electr√≥nicos de SECOP II, desde la ingesta y depuraci√≥n hasta el despliegue en un escenario de ‚Äúproducci√≥n‚Äù. El trabajo no se limit√≥ a entrenar modelos: se estandariz√≥ un entorno de ejecuci√≥n (Docker + Spark), se consolid√≥ un dataset curado en Parquet y se dej√≥ un pipeline de Machine Learning escalable que permite repetir el proceso con nuevos cortes de datos sin rehacer cada paso manualmente.

En la fase exploratoria qued√≥ claro que el valor contractual es una variable de alta dispersi√≥n y con asimetr√≠a marcada (muchos contratos de valores relativamente bajos y una fracci√≥n peque√±a de contratos con montos muy altos). Ese comportamiento justific√≥ decisiones t√©cnicas posteriores del pipeline: limpieza de registros inconsistentes, tratamiento de nulos, y el uso de transformaciones que estabilizan la escala de la variable objetivo (incluyendo el uso de una versi√≥n transformada del valor para mejorar el ajuste y la estabilidad del entrenamiento). A nivel descriptivo, tambi√©n se identificaron patrones estructurales √∫tiles para interpretaci√≥n: concentraci√≥n territorial y diferencias claras por modalidad y estado contractual, que ayudan a contextualizar cualquier salida predictiva y a evitar interpretaciones ‚Äúciegas‚Äù del modelo.

En t√©rminos de modelado, el valor principal del trabajo fue pasar de un enfoque b√°sico a uno robusto. Se implementaron pipelines con feature engineering (codificaci√≥n de categor√≠as y ensamblado de variables), escalamiento y reducci√≥n de dimensionalidad (PCA) para dejar un conjunto de features consistente y entrenable de manera distribuida. Sobre esa base se entrenaron modelos de regresi√≥n y clasificaci√≥n, se evaluaron con m√©tricas acordes al objetivo (errores para regresi√≥n y desempe√±o discriminativo para clasificaci√≥n), y se control√≥ el riesgo de sobreajuste comparando comportamiento entre entrenamiento y prueba. La regularizaci√≥n y el ajuste de hiperpar√°metros se trabajaron como parte del m√©todo (no como ‚Äúreto aislado‚Äù): se explor√≥ c√≥mo cambian los errores al ajustar penalizaciones y se formaliz√≥ la selecci√≥n del mejor enfoque mediante validaci√≥n cruzada, priorizando generalizaci√≥n por encima de resultados ‚Äúbonitos‚Äù solo en entrenamiento.

Finalmente, el componente MLOps cerr√≥ el ciclo de vida del modelo. Se registraron experimentos y resultados en MLflow, se dejaron versiones trazables en el Model Registry y se plante√≥ el uso de stages (Staging/Production) como control pr√°ctico antes de exponer un modelo como definitivo. Con esto, el notebook de inferencia en producci√≥n no queda como demostraci√≥n te√≥rica: representa un patr√≥n realista para cargar el modelo por nombre y etapa, generar predicciones batch, agregar metadatos (timestamp) y guardar salidas en formatos consumibles (Parquet/CSV). En conjunto, el entregable final no es solo ‚Äúun modelo‚Äù, sino un sistema reproducible y escalable que deja bases s√≥lidas para monitoreo, retraining y mejoras futuras con nuevas variables o nuevas ventanas de datos.






