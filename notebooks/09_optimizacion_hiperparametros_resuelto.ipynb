{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11cd93d-cb8a-4a43-ab32-7c67c28244fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/12 02:45:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sesión Spark iniciada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 80,062\n",
      "Test: 19,938\n",
      "✓ Modelo base y evaluador configurados\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/12 02:45:28 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# %%\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_HyperparameterTuning\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ Sesión Spark iniciada\")\n",
    "\n",
    "# base de datos\n",
    "df = spark.read.parquet(\"/opt/spark-data/raw/secop_ml_ready.parquet\")\n",
    "\n",
    "df = df.withColumnRenamed(\"valor_del_contrato_num\", \"label\") \\\n",
    "       .withColumnRenamed(\"features_pca\", \"features\") \\\n",
    "       .filter(col(\"label\").isNotNull())\n",
    "\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Train: {train.count():,}\")\n",
    "print(f\"Test: {test.count():,}\")\n",
    "\n",
    "# Modelo base y evaluador\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "print(\"✓ Modelo base y evaluador configurados\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea75f48-651f-4703-97d1-18531d6fae38",
   "metadata": {},
   "source": [
    "### **Diseño del Grid de Hiperparámetros**\r\n",
    "\r\n",
    "**¿Por qué usamos escala logarítmica para regParam (0.01, 0.1, 1.0) en lugar de lineal (0.33, 0.66, 1.0)?**\r\n",
    "\r\n",
    "Usamos escala logarítmica porque la regularización afecta el modelo de manera exponencial y no lineal. \r\n",
    "Pequeños cambios en valores bajos de λ pueden generar cambios significativos en los coeficientes.\r\n",
    "\r\n",
    "Explorar valores como 0.01, 0.1 y 1.0 permite cubrir diferentes órdenes de magnitud y detectar rápidamente\r\n",
    "si el modelo necesita poca, moderada o alta regularización.\r\n",
    "\r\n",
    "Una escala lineal podría concentrarse en una zona poco informativa y no capturar correctamente el impacto real\r\n",
    "de la regularización.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b24b8c-5cca-4601-98d9-e40c9bb6e317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones totales: 27\n",
      "Modelos totales a entrenar con K=3: 81\n"
     ]
    }
   ],
   "source": [
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [50, 100, 200]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Combinaciones totales: {len(grid)}\")\n",
    "\n",
    "K = 3\n",
    "print(f\"Modelos totales a entrenar con K={K}: {len(grid) * K}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428025a-77e7-4168-8255-09b59b0dbc43",
   "metadata": {},
   "source": [
    "Combinaciones totales generadas: 27  \r\n",
    "(3 valores de regParam × 3 de elasticNetParam × 3 de maxIter)\r\n",
    "\r\n",
    "Si usamos K=3 folds:\r\n",
    "\r\n",
    "Total modelos entrenados = 27 × 3 = 81 modelos\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af073b-6846-45a9-b952-52cd267cacf4",
   "metadata": {},
   "source": [
    "### **Implementar Grid Search + Cross-Validation**\r\n",
    "\r\n",
    "Usamos K=3 en lugar de K=5 porque:\r\n",
    "\r\n",
    "- Reduce el tiempo computacional significativamente.\r\n",
    "- Ya estamos explorando múltiples combinaciones (27 en este caso).\r\n",
    "- El dataset es grande (~100k registros), por lo que K=3 ofrece un buen balance entre robustez y costo computacional.\r\n",
    "- K=5 aumentaría el tiempo de entrenamiento en un 66% adicional sin necesariamente mejorar sustancialmente la estimación.\r\n",
    "\r\n",
    "En escenarios de tuning amplio, es recomendable comenzar con K pequeño y luego refinar con K mayor si es necesario.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d07ca06-0bf4-43e1-93ed-81fa5b4a5e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Grid Search + Cross-Validation (K=3)...\n",
      "✓ Grid Search + CV completado en 18.63 segundos\n",
      "\n",
      "=== MEJOR MODELO (Grid Search + CV) ===\n",
      "regParam:        1.0\n",
      "elasticNetParam: 1.0\n",
      "maxIter:         50\n",
      "RMSE en Test:    $2,342,402,953.27\n"
     ]
    }
   ],
   "source": [
    "# Configurar CrossValidator con K=3\n",
    "cv_grid = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Entrenando Grid Search + Cross-Validation (K=3)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "cv_grid_model = cv_grid.fit(train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Grid Search + CV completado en {grid_time:.2f} segundos\")\n",
    "\n",
    "# %%\n",
    "best_grid_model = cv_grid_model.bestModel\n",
    "\n",
    "predictions_grid = best_grid_model.transform(test)\n",
    "rmse_grid = evaluator.evaluate(predictions_grid)\n",
    "\n",
    "print(\"\\n=== MEJOR MODELO (Grid Search + CV) ===\")\n",
    "print(f\"regParam:        {best_grid_model.getRegParam()}\")\n",
    "print(f\"elasticNetParam: {best_grid_model.getElasticNetParam()}\")\n",
    "print(f\"maxIter:         {best_grid_model.getMaxIter()}\")\n",
    "print(f\"RMSE en Test:    ${rmse_grid:,.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0459e2-fee0-4112-b8af-d83f2871abbd",
   "metadata": {},
   "source": [
    "### **Implementar Train-Validation Split**\n",
    "\n",
    "En esta estrategia se divide el conjunto de entrenamiento en un único split (80% entrenamiento y 20% validación), en lugar de usar K folds.\n",
    "\n",
    "Ventajas:\n",
    "- Mucho más rápido que Cross-Validation.\n",
    "- Cada combinación se entrena solo una vez.\n",
    "\n",
    "Desventajas:\n",
    "- La métrica depende de un único split.\n",
    "- Puede ser menos estable si la partición no es representativa.\n",
    "\n",
    "Usamos trainRatio = 0.8 porque:\n",
    "- Es el estándar clásico (80/20).\n",
    "- Mantiene suficiente información para entrenar.\n",
    "- Permite validar sin desperdiciar demasiados datos.emasiados datos.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b827fd-5fbf-4399-b3b4-13cc7d56852d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con Train-Validation Split...\n",
      "✓ Train-Validation completado en 6.66 segundos\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Entrenando con Train-Validation Split...\")\n",
    "\n",
    "start_time = time.time()\n",
    "tvs_model = tvs.fit(train)\n",
    "tvs_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Train-Validation completado en {tvs_time:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb85e651-1ec5-4cd1-82ea-19a7163a21df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MEJOR MODELO (Train-Validation Split) ===\n",
      "regParam:        1.0\n",
      "elasticNetParam: 1.0\n",
      "maxIter:         50\n",
      "RMSE en Test:    $2,342,402,953.27\n"
     ]
    }
   ],
   "source": [
    "# Obtener Mejor Modelo y Evaluar en Test\n",
    "best_tvs_model = tvs_model.bestModel\n",
    "\n",
    "predictions_tvs = best_tvs_model.transform(test)\n",
    "rmse_tvs = evaluator.evaluate(predictions_tvs)\n",
    "\n",
    "print(\"\\n=== MEJOR MODELO (Train-Validation Split) ===\")\n",
    "print(f\"regParam:        {best_tvs_model.getRegParam()}\")\n",
    "print(f\"elasticNetParam: {best_tvs_model.getElasticNetParam()}\")\n",
    "print(f\"maxIter:         {best_tvs_model.getMaxIter()}\")\n",
    "print(f\"RMSE en Test:    ${rmse_tvs:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbf2a1-5e43-4901-a9ab-1e03a47a2af1",
   "metadata": {},
   "source": [
    "### **Comparar ambas estrategias (rendimiento vs velocidad)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2fc5492-41dc-475d-8b7a-c3f9f3f791e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARACIÓN DE ESTRATEGIAS\n",
      "============================================================\n",
      "Grid Search + CV:\n",
      "  - Tiempo: 18.63s\n",
      "  - RMSE Test: $2,342,402,953.27\n",
      "  - Hiperparámetros: λ=1.0, α=1.0, maxIter=50\n",
      "\n",
      "Train-Validation Split:\n",
      "  - Tiempo: 6.66s\n",
      "  - RMSE Test: $2,342,402,953.27\n",
      "  - Hiperparámetros: λ=1.0, α=1.0, maxIter=50\n",
      "\n",
      "Diferencia RMSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPARACIÓN DE ESTRATEGIAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Grid Search + CV:\")\n",
    "print(f\"  - Tiempo: {grid_time:.2f}s\")\n",
    "print(f\"  - RMSE Test: ${rmse_grid:,.2f}\")\n",
    "print(f\"  - Hiperparámetros: λ={best_grid_model.getRegParam()}, \"\n",
    "      f\"α={best_grid_model.getElasticNetParam()}, \"\n",
    "      f\"maxIter={best_grid_model.getMaxIter()}\")\n",
    "\n",
    "print(\"\\nTrain-Validation Split:\")\n",
    "print(f\"  - Tiempo: {tvs_time:.2f}s\")\n",
    "print(f\"  - RMSE Test: ${rmse_tvs:,.2f}\")\n",
    "print(f\"  - Hiperparámetros: λ={best_tvs_model.getRegParam()}, \"\n",
    "      f\"α={best_tvs_model.getElasticNetParam()}, \"\n",
    "      f\"maxIter={best_tvs_model.getMaxIter()}\")\n",
    "\n",
    "print(\"\\nDiferencia RMSE:\", abs(rmse_grid - rmse_tvs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44c6f3-c0f4-42f4-8d7d-c88d7b18a3b7",
   "metadata": {},
   "source": [
    "En este experimento se compararon dos estrategias de optimización de hiperparámetros:\n",
    "\n",
    "1. **Grid Search + Cross-Validation (K=3)**\n",
    "2. **Train-Validation Split (80/20)**\n",
    "\n",
    "**Resultados obtenidos:**\n",
    "\n",
    "- Grid Search + CV:\n",
    "  - Tiempo: 18.63 segundos\n",
    "  - RMSE Test: $2,342,402,953.27\n",
    "  - Hiperparámetros óptimos: λ=1.0, α=1.0, maxIter=50\n",
    "\n",
    "- Train-Validation Split:\n",
    "  - Tiempo: 6.66 segundos\n",
    "  - RMSE Test: $2,342,402,953.27\n",
    "  - Hiperparámetros óptimos: λ=1.0, α=1.0, maxIter=50\n",
    "\n",
    "**¿Cuándo usar cada estrategia?**\n",
    "\n",
    "- Usaría Grid Search + CV cuando:\n",
    "  - El dataset es pequeño o mediano.\n",
    "  - El problema es crítico y necesito mayor robustez.\n",
    "  - Hay alta variabilidad en los resultados.\n",
    "\n",
    "- Usaría Train-Validation Split cuando:\n",
    "  - El dataset es grande.\n",
    "  - Necesito rapidez.\n",
    "  - El grid es amplio y el costo computacional es alto.\n",
    "\n",
    "En entornos productivos con Big Data, Train-Validation Split suele ser la opción más eficiente, mientras que Cross-Validation\n",
    "es preferible en fases exploratorias o académicas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Seleccionar y guardar modelo final con hiperparametros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08c79c9-8f44-472c-b59d-512bec7f05ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrategia seleccionada: Grid Search + CV\n",
      "RMSE final seleccionado: $2,342,402,953.27\n",
      "✓ Modelo final guardado en: /opt/spark-data/raw/tuned_model\n",
      "✓ Hiperparámetros óptimos guardados en: /opt/spark-data/raw/hiperparametros_optimos.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Seleccionar mejor modelo global\n",
    "mejor_modelo = best_grid_model if rmse_grid <= rmse_tvs else best_tvs_model\n",
    "estrategia_usada = \"Grid Search + CV\" if rmse_grid <= rmse_tvs else \"Train-Validation Split\"\n",
    "rmse_final = rmse_grid if rmse_grid <= rmse_tvs else rmse_tvs\n",
    "\n",
    "print(f\"Estrategia seleccionada: {estrategia_usada}\")\n",
    "print(f\"RMSE final seleccionado: ${rmse_final:,.2f}\")\n",
    "\n",
    "# Guardar modelo\n",
    "model_path = \"/opt/spark-data/raw/tuned_model\"\n",
    "mejor_modelo.write().overwrite().save(model_path)\n",
    "\n",
    "print(f\"✓ Modelo final guardado en: {model_path}\")\n",
    "\n",
    "# hiperparametros optimos\n",
    "hiperparametros_optimos = {\n",
    "    \"regParam\": float(mejor_modelo.getRegParam()),\n",
    "    \"elasticNetParam\": float(mejor_modelo.getElasticNetParam()),\n",
    "    \"maxIter\": int(mejor_modelo.getMaxIter()),\n",
    "    \"rmse_test\": float(rmse_final),\n",
    "    \"estrategia\": estrategia_usada\n",
    "}\n",
    "\n",
    "json_path = \"/opt/spark-data/raw/hiperparametros_optimos.json\"\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(hiperparametros_optimos, f, indent=4)\n",
    "\n",
    "print(f\"✓ Hiperparámetros óptimos guardados en: {json_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab087961-1dd7-49f9-bb24-d030fb42c0f7",
   "metadata": {},
   "source": [
    "### **Refinar grid alrededor de mejores valores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "841deb7d-c46b-4175-ae6e-a46a12c4a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones grid fino: 30\n",
      "Modelos a entrenar con K=3: 90\n",
      "\n",
      "Entrenando Grid Fino + CV...\n",
      "✓ Grid fino completado en 20.88 segundos\n"
     ]
    }
   ],
   "source": [
    "# Definir grid fino\n",
    "fine_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.7, 0.9, 1.0, 1.1, 1.3]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.8, 0.9, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [50, 75]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Combinaciones grid fino: {len(fine_grid)}\")\n",
    "print(f\"Modelos a entrenar con K=3: {len(fine_grid) * 3}\")\n",
    "\n",
    "# CrossValidator con grid fino\n",
    "cv_fine = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=fine_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Grid Fino + CV...\")\n",
    "start_time = time.time()\n",
    "cv_fine_model = cv_fine.fit(train)\n",
    "fine_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Grid fino completado en {fine_time:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daef885d-93c6-44c8-b40a-a575ad4c83ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MEJOR MODELO (Grid Fino) ===\n",
      "regParam:        0.9\n",
      "elasticNetParam: 0.9\n",
      "maxIter:         50\n",
      "RMSE Test:       $2,342,402,953.42\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "best_fine_model = cv_fine_model.bestModel\n",
    "\n",
    "preds_fine = best_fine_model.transform(test)\n",
    "rmse_fine = evaluator.evaluate(preds_fine)\n",
    "\n",
    "print(\"\\n=== MEJOR MODELO (Grid Fino) ===\")\n",
    "print(f\"regParam:        {best_fine_model.getRegParam()}\")\n",
    "print(f\"elasticNetParam: {best_fine_model.getElasticNetParam()}\")\n",
    "print(f\"maxIter:         {best_fine_model.getMaxIter()}\")\n",
    "print(f\"RMSE Test:       ${rmse_fine:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b02486-fd5a-4489-91c9-cbafa9de3680",
   "metadata": {},
   "source": [
    "## Preguntas de Reflexión\r\n",
    "\r\n",
    "1. **¿Cuándo usarías Grid Search vs Random Search?**\r\n",
    "\r\n",
    "   **Respuesta:**\r\n",
    "   \r\n",
    "   Usaría **Grid Search** cuando el espacio de hiperparámetros es pequeño o moderado y quiero explorar todas las combinaciones posibles de forma exhaustiva. Es ideal cuando el número de parámetros es reducido y el costo computacional es manejable.\r\n",
    "   \r\n",
    "   Usaría **Random Search** cuando el espacio de búsqueda es grande o continuo, ya que permite explorar más combinaciones distintas en menos tiempo. Random Search suele ser más eficiente en alta dimensionalidad, ya que no desperdicia recursos evaluando combinaciones poco prometedoras de manera sistemática.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "2. **¿Por qué Train-Validation Split es más rápido que CV?**\r\n",
    "\r\n",
    "   **Respuesta:**\r\n",
    "   \r\n",
    "   Train-Validation Split es más rápido porque cada combinación de hiperparámetros se entrena **una sola vez**, utilizando un único split interno (por ejemplo 80/20).  \r\n",
    "   \r\n",
    "   En cambio, Cross-Validation con K folds entrena cada combinación **K veces**, lo que multiplica el costo computacional por K. Por eso CV es más robusto, pero también más costoso en tiempo y recursos.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "3. **¿Qué pasa si el grid es demasiado grande?**\r\n",
    "\r\n",
    "   **Respuesta:**\r\n",
    "   \r\n",
    "   Si el grid es demasiado grande:\r\n",
    "   \r\n",
    "   - El tiempo de entrenamiento crece exponencialmente.\r\n",
    "   - Puede saturar memoria y recursos del clúster.\r\n",
    "   - Se vuelve ineficiente explorar combinaciones poco relevantes.\r\n",
    "   \r\n",
    "   En entornos Big Data, un grid muy amplio puede volver el proceso inviable. Por eso es recomendable:\r\n",
    "   - Usar escala logarítmica.\r\n",
    "   - Refinar alrededor de las mejores zonas.\r\n",
    "   - Reducir dimensionalidad antes del tuning.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "4. **¿Cómo implementarías Random Search en Spark ML?**\r\n",
    "   (Spark ML no tiene Random Search nativo)\r\n",
    "\r\n",
    "   **Respuesta:**\r\n",
    "   \r\n",
    "   Para implementar Random Search en Spark ML se podría:\r\n",
    "   \r\n",
    "   1. Definir rangos continuos para los hiperparámetros.\r\n",
    "   2. Generar combinaciones aleatorias usando `random` o `numpy`.\r\n",
    "   3. Construir manualmente un `ParamGridBuilder` con esas combinaciones aleatorias.\r\n",
    "   4. Pasarlas al `CrossValidator` o `TrainValidationSplit`.\r\n",
    "   \r\n",
    "   Es decir, simular Random Search creando un grid con muestras aleatorias en lugar de combinaciones exhaustivas. Esto permite explorar el espacio de búsqueda de manera más eficiente cuando los hiperparámetros son muchos o continuos.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ed428d7-c4a1-474f-9714-df8fef15ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
