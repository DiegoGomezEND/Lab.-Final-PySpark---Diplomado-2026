{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2796b31-f3ef-4caa-8961-eb6c3fbed74f",
   "metadata": {},
   "source": [
    "## Notebook 03: Feature Engineering con Pipelines\n",
    "\n",
    "**Sección 13 - Spark ML**: Construcción de pipelines end-to-end\n",
    "\n",
    "**Objetivo**: Aplicar VectorAssembler y construir un pipeline de transformación.\n",
    "\n",
    "**Conceptos clave**:\n",
    "\n",
    "  - **Transformer**: Aplica transformaciones (ej: StringIndexer)\n",
    "  - **Estimator**: Aprende de los datos y genera un modelo\n",
    "  - **Pipeline**: Encadena múltiples stages secuencialmente\n",
    "\n",
    "**Actividades:**\n",
    "  1. Crear StringIndexer para variables categóricas\n",
    "  2. Aplicar OneHotEncoder\n",
    "  3. Combinar features con VectorAssembler\n",
    "  4. Construir y ejecutar Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95492266-b3ef-4845-aecc-b9a67f4d221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/13 15:05:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/13 15:05:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, when, isnull\n",
    "\n",
    "# %%\n",
    "# Configurar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_FeatureEngineering\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d550b11c-f53b-48ef-8f9f-1fcf222fa970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 15:05:21 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros cargados: 100,000\n",
      "Columnas disponibles:\n",
      "  - referencia_del_contrato\n",
      "  - valor_del_contrato\n",
      "  - valor_del_contrato_num\n",
      "  - departamento\n",
      "  - tipo_de_contrato\n",
      "  - fecha_de_firma\n",
      "  - fecha_de_firma_ts\n",
      "  - duraci_n_del_contrato\n",
      "  - proveedor_adjudicado\n",
      "  - estado_contrato\n",
      "  - valor_del_contrato_log\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "df = spark.read.parquet(\"/opt/spark-data/raw/secop_eda.parquet\")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Antes de entrenar/transformar (o después, da igual si la columna existe)\n",
    "df = df.withColumn(\"valor_del_contrato_log\", F.log1p(F.col(\"valor_del_contrato_num\")))\n",
    "print(f\"Registros cargados: {df.count():,}\")\n",
    "\n",
    "# %%\n",
    "# Explorar columnas disponibles\n",
    "print(\"Columnas disponibles:\")\n",
    "for col_name in df.columns:\n",
    "    print(f\"  - {col_name}\")\n",
    "\n",
    "\n",
    "categorical_cols = [\n",
    "    \"departamento\",\n",
    "    \"tipo_de_contrato\",\n",
    "    \"estado_contrato\"\n",
    "]\n",
    "numeric_cols = [\n",
    "    \"valor_del_contrato_num\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e8e9b-a6b5-471d-afd6-dd710827ca7c",
   "metadata": {},
   "source": [
    "### **Seleccionar features categóricas y numéricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d697026-0808-4077-bd65-8f5bd3f14c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categóricas seleccionadas: ['departamento', 'tipo_de_contrato', 'estado_contrato']\n",
      "Numéricas seleccionadas: ['valor_del_contrato_log']\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [\n",
    "    \"departamento\",\n",
    "    \"tipo_de_contrato\",\n",
    "    \"estado_contrato\"\n",
    "]\n",
    "numeric_cols = [\n",
    "    \"valor_del_contrato_log\"\n",
    "]\n",
    "\n",
    "available_cat = [c for c in categorical_cols if c in df.columns]\n",
    "available_num = [c for c in numeric_cols if c in df.columns]\n",
    "\n",
    "print(f\"Categóricas seleccionadas: {available_cat}\")\n",
    "print(f\"Numéricas seleccionadas: {available_num}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d1f980-43ee-47ad-b3c4-73f232f4522a",
   "metadata": {},
   "source": [
    "## **Implementar estrategia de limpieza de datos**\n",
    "\n",
    "En analisis previos no se encontraron datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e707dbb-3594-4903-86ed-d80de1ae218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros antes de limpiar: 100,000\n",
      "Registros después de limpiar: 100,000\n"
     ]
    }
   ],
   "source": [
    "# Estrategia: eliminar filas con valores nulos en features seleccionadas\n",
    "\n",
    "df_clean = df.dropna(subset=available_cat + available_num)\n",
    "\n",
    "print(f\"Registros antes de limpiar: {df.count():,}\")\n",
    "print(f\"Registros después de limpiar: {df_clean.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e5092-3709-4075-892b-f5af0086da26",
   "metadata": {},
   "source": [
    "## **Crear VectorAssembler para combinar features**\n",
    "\n",
    "Se buscara combinar variables numéricas y categóricas codificadas en un solo vector, para que ese vector numérico contenga todas las variables explicativas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a0201a-1b92-4602-8e5c-10aed3f473f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexers creados:\n",
      " - departamento → departamento_idx\n",
      " - tipo_de_contrato → tipo_de_contrato_idx\n",
      " - estado_contrato → estado_contrato_idx\n"
     ]
    }
   ],
   "source": [
    "# StringIndexer para variables categóricas\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=f\"{c}_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    for c in available_cat\n",
    "]\n",
    "\n",
    "print(\"StringIndexers creados:\")\n",
    "for idx in indexers:\n",
    "    print(f\" - {idx.getInputCol()} → {idx.getOutputCol()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed97fee0-6161-4c82-8f59-80920e6f0e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OneHotEncoders creados:\n",
      " - departamento_idx → departamento_vec\n",
      " - tipo_de_contrato_idx → tipo_de_contrato_vec\n",
      " - estado_contrato_idx → estado_contrato_vec\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder para variables categóricas\n",
    "encoders = [\n",
    "    OneHotEncoder(\n",
    "        inputCol=f\"{c}_idx\",\n",
    "        outputCol=f\"{c}_vec\"\n",
    "    )\n",
    "    for c in available_cat\n",
    "]\n",
    "\n",
    "print(\"\\nOneHotEncoders creados:\")\n",
    "for enc in encoders:\n",
    "    print(f\" - {enc.getInputCol()} → {enc.getOutputCol()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf40ff2-49a9-4856-b276-ed0ae8161965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas que se combinarán en el vector de features:\n",
      " - valor_del_contrato_log\n",
      " - departamento_vec\n",
      " - tipo_de_contrato_vec\n",
      " - estado_contrato_vec\n"
     ]
    }
   ],
   "source": [
    "# Columnas categóricas codificadas (salida del OneHotEncoder)\n",
    "encoded_cat_cols = [c + \"_vec\" for c in available_cat]\n",
    "\n",
    "# ensamblar\n",
    "feature_cols = available_num + encoded_cat_cols\n",
    "print(\"Columnas que se combinarán en el vector de features:\")\n",
    "\n",
    "for c in feature_cols:\n",
    "    print(f\" - {c}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dce2c0-c005-48b7-85df-1b6208caecfb",
   "metadata": {},
   "source": [
    "### **Construir Pipeline completo (orden correcto de stages)**\n",
    "\n",
    "StringIndexer  →  OneHotEncoder  →  VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d830fc-d6fa-402a-ba64-f9eb3e019165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline construido con 7 stages:\n",
      " Stage 1: StringIndexer\n",
      " Stage 2: StringIndexer\n",
      " Stage 3: StringIndexer\n",
      " Stage 4: OneHotEncoder\n",
      " Stage 5: OneHotEncoder\n",
      " Stage 6: OneHotEncoder\n",
      " Stage 7: VectorAssembler\n",
      "\n",
      "Entrenando pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline entrenado\n",
      "Transformación aplicada\n",
      "root\n",
      " |-- features_raw: vector (nullable = true)\n",
      "\n",
      "Dimensión del vector de features: 62\n"
     ]
    }
   ],
   "source": [
    "pipeline_stages = []\n",
    "pipeline_stages.extend(indexers)\n",
    "pipeline_stages.extend(encoders)\n",
    "pipeline_stages.append(assembler)\n",
    "\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "print(f\"\\nPipeline construido con {len(pipeline_stages)} stages:\")\n",
    "for i, stage in enumerate(pipeline_stages, start=1):\n",
    "    print(f\" Stage {i}: {type(stage).__name__}\")\n",
    "\n",
    "# Entrenar y aplicar el Pipeline\n",
    "print(\"\\nEntrenando pipeline...\")\n",
    "pipeline_model = pipeline.fit(df_clean)\n",
    "print(\"Pipeline entrenado\")\n",
    "\n",
    "df_transformed = pipeline_model.transform(df_clean)\n",
    "print(\"Transformación aplicada\")\n",
    "\n",
    "# Verificar el vector final\n",
    "df_transformed.select(\"features_raw\").printSchema()\n",
    "\n",
    "sample = df_transformed.select(\"features_raw\").first()[0]\n",
    "print(f\"Dimensión del vector de features: {len(sample)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabe7ef-3ea7-48a1-96c0-3be930d0c22d",
   "metadata": {},
   "source": [
    "### **Calcular dimension total de features post-encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d96e644c-3827-4a6c-bf63-9d3243637621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORÍAS POR VARIABLE CATEGÓRICA\n",
      "departamento: 34 categorías únicas\n",
      "tipo_de_contrato: 20 categorías únicas\n",
      "estado_contrato: 7 categorías únicas\n",
      "\n",
      "Total features categóricas (OneHot): 61\n",
      "Features numéricas: 1\n",
      "FEATURES TOTALES ESPERADAS: 62\n",
      "\n",
      "Dimensión real del vector features_raw: 62\n"
     ]
    }
   ],
   "source": [
    "print(\"CATEGORÍAS POR VARIABLE CATEGÓRICA\")\n",
    "\n",
    "total_ohe_features = 0\n",
    "\n",
    "for cat in available_cat:\n",
    "    n_cat = df_clean.select(cat).distinct().count()\n",
    "    total_ohe_features += n_cat\n",
    "    print(f\"{cat}: {n_cat} categorías únicas\")\n",
    "\n",
    "print(f\"\\nTotal features categóricas (OneHot): {total_ohe_features}\")\n",
    "print(f\"Features numéricas: {len(available_num)}\")\n",
    "print(f\"FEATURES TOTALES ESPERADAS: {total_ohe_features + len(available_num)}\")\n",
    "\n",
    "# Validacion contra el vector real\n",
    "sample = df_transformed.select(\"features_raw\").first()[0]\n",
    "print(f\"\\nDimensión real del vector features_raw: {len(sample)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747c52b-491d-47fe-b50c-65167c3b84d6",
   "metadata": {},
   "source": [
    "La dimensión final del vector de características se obtuvo sumando una variable numérica continua y las variables categóricas codificadas mediante OneHotEncoding. Cada categoría única genera una dimensión independiente, lo que resultó en un vector final de 62 features, coincidente con el conteo empírico obtenido tras aplicar el pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba3b41-2930-4a79-9930-1c4c5b47fe31",
   "metadata": {},
   "source": [
    "### **Analisis de varianza de features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279e2b53-f4e9-4861-b64c-f6642744ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANÁLISIS DE VARIANZA DE FEATURES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de features: (1000, 62)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"ANÁLISIS DE VARIANZA DE FEATURES\")\n",
    "\n",
    "# Tomamos una muestra para no explotar memoria\n",
    "sample_df = (\n",
    "    df_transformed\n",
    "    .select(\"features_raw\")\n",
    "    .sample(fraction=0.01, seed=42)\n",
    "    .limit(1000)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Convertir vector Spark → array NumPy\n",
    "features_matrix = np.array(\n",
    "    [row[\"features_raw\"].toArray() for _, row in sample_df.iterrows()]\n",
    ")\n",
    "\n",
    "print(f\"Matriz de features: {features_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52dc46d4-2dfc-497b-a1c7-dec95e03a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features con mayor varianza:\n",
      "Feature 0: varianza = 6.1991\n",
      "Feature 35: varianza = 0.1948\n",
      "Feature 1: varianza = 0.1699\n",
      "Feature 55: varianza = 0.1618\n",
      "Feature 2: varianza = 0.1275\n"
     ]
    }
   ],
   "source": [
    "variances = np.var(features_matrix, axis=0)\n",
    "\n",
    "# Top 5 features con mayor varianza\n",
    "top_5_idx = np.argsort(variances)[-5:][::-1]\n",
    "\n",
    "print(\"\\nTop 5 features con mayor varianza:\")\n",
    "for idx in top_5_idx:\n",
    "    print(f\"Feature {idx}: varianza = {variances[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42f57d-83ca-496e-913f-4064e0ce55e0",
   "metadata": {},
   "source": [
    "Se realizó un análisis de varianza sobre el vector de características para identificar las variables con mayor dispersión. Las features con mayor varianza corresponden principalmente a variables categóricas con alta frecuencia diferencial y a la variable numérica continua, validando la necesidad de aplicar técnicas de normalización y reducción dimensional en etapas posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a23541-2b4a-46b9-9eda-f8a667299917",
   "metadata": {},
   "source": [
    "### **Cierre del Pipelines y Feature Engineering: Guardar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25cce2d-7b49-4add-b612-84e0940ddef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con features guardado en: /opt/spark-data/raw/secop_features_1.parquet\n",
      "✓ Registros: 100,000\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/opt/spark-data/raw/secop_features_1.parquet\"\n",
    "\n",
    "df_transformed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(f\"Dataset con features guardado en: {output_path}\")\n",
    "print(f\"✓ Registros: {df_transformed.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0336895-2d2f-4e1e-bd4f-16fde72afcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64d0ed-d3da-4421-b98b-29ade23876f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
