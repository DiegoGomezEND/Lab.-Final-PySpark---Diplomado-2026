{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7896ec16-86a8-443f-bea7-576c85165fb2",
   "metadata": {},
   "source": [
    "## Notebook 01: Ingesta de Datos\r\n",
    "**Objetivo de la Ingesta:** Descargar y cargar datos de contratos públicos desde la API de Datos Abiertos Colombia (SECOP II).\n",
    "\n",
    "**Dataset:** SSECOP II - Contratos Electrónicos.)\n",
    "\n",
    "**Fuente:** \"https://www.datos.gov.co/Gastos-Gubernamentales/SECOP-II-Contratos-Electr-nicos/jbjy-vk9h\"\n",
    "\n",
    "**Actividades:**\n",
    "  - Configurar SparkSession conectada al cluster\n",
    "  - Descargar datos desde la API Socrata (SECOP II)\n",
    "  - Cargar datos en Spark y explorar el esquema\n",
    "  - Seleccionar columnas clave para ML\n",
    "  - Guardar en formato Parquet optimizado\n",
    "izadouet optimizadot optimizado\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfe25c4-ddab-406b-9923-6662d1e1893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sodapy in /usr/local/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests>=2.28.1 in /usr/local/lib/python3.11/site-packages (from sodapy) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (2026.1.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b94581-47f6-4e58-b028-c19be30268f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "# Para control de la ingesta\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "from delta import *\n",
    "import os\n",
    "from sodapy import Socrata\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a524d4e-19f2-4040-bdb6-ae7ce9c12d4c",
   "metadata": {},
   "source": [
    "## **Configurar SparkSession**\n",
    "\n",
    "### Conectamos al Spark Master del cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8ea947-ef9e-48d5-a079-a833ce9f065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b12cb993-1bda-4fa1-8d4d-fa093b533f0c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 181ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b12cb993-1bda-4fa1-8d4d-fa093b533f0c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "26/02/07 19:03:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Spark Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "master_url = \"spark://spark-master:7077\"\n",
    "\n",
    "# Configuración y Añadimos Delta Lake\n",
    "# Despues de master_url\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Lab_SECOP_Bronze\") \\\n",
    "    .master(master_url) \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd6981-cfdf-40f1-94a1-03faf96ac36d",
   "metadata": {},
   "source": [
    "## **Descargar datos desde API Socrata**\n",
    "\n",
    "Se seleccionaran los datos registrados en la pagina de SECOP desde el 1 de enero de 2026 para asi mismo ver que contratos fueron presentados en el primer mes de año 2026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d0f0f7-3be6-4c49-aee7-00c83b595e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo datos desde API Socrata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/07 19:03:49 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros descargados: 100,000\n"
     ]
    }
   ],
   "source": [
    "print(\"Extrayendo datos desde API Socrata\")\n",
    "\n",
    "client = Socrata(\"www.datos.gov.co\", None)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "WHERE fecha_de_firma IS NOT NULL\n",
    "ORDER BY fecha_de_firma DESC\n",
    "LIMIT 100000\n",
    "\"\"\"\n",
    "\n",
    "results = client.get(\n",
    "    \"jbjy-vk9h\",\n",
    "    query=query\n",
    ")\n",
    "\n",
    "print(f\"Registros descargados: {len(results):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93cb92e8-bb30-4282-b5f0-eacd3a027376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas cargadas:\n",
      "['nombre_entidad', 'nit_entidad', 'departamento', 'ciudad', 'localizaci_n', 'orden', 'sector', 'rama', 'entidad_centralizada', 'proceso_de_compra', 'id_contrato', 'referencia_del_contrato', 'estado_contrato', 'codigo_de_categoria_principal', 'descripcion_del_proceso', 'tipo_de_contrato', 'modalidad_de_contratacion', 'justificacion_modalidad_de', 'fecha_de_firma', 'fecha_de_fin_del_contrato', 'condiciones_de_entrega', 'tipodocproveedor', 'documento_proveedor', 'proveedor_adjudicado', 'es_grupo', 'es_pyme', 'habilita_pago_adelantado', 'liquidaci_n', 'obligaci_n_ambiental', 'obligaciones_postconsumo', 'reversion', 'origen_de_los_recursos', 'destino_gasto', 'valor_del_contrato', 'valor_de_pago_adelantado', 'valor_facturado', 'valor_pendiente_de_pago', 'valor_pagado', 'valor_amortizado', 'valor_pendiente_de', 'valor_pendiente_de_ejecucion', 'estado_bpin', 'c_digo_bpin', 'anno_bpin', 'saldo_cdp', 'saldo_vigencia', 'espostconflicto', 'dias_adicionados', 'puntos_del_acuerdo', 'pilares_del_acuerdo', 'urlproceso', 'nombre_representante_legal', 'nacionalidad_representante_legal', 'domicilio_representante_legal', 'tipo_de_identificaci_n_representante_legal', 'identificaci_n_representante_legal', 'g_nero_representante_legal', 'presupuesto_general_de_la_nacion_pgn', 'sistema_general_de_participaciones', 'sistema_general_de_regal_as', 'recursos_propios_alcald_as_gobernaciones_y_resguardos_ind_genas_', 'recursos_de_credito', 'recursos_propios', 'codigo_entidad', 'codigo_proveedor', 'objeto_del_contrato', 'duraci_n_del_contrato', 'nombre_del_banco', 'tipo_de_cuenta', 'n_mero_de_cuenta', 'el_contrato_puede_ser_prorrogado', 'nombre_ordenador_del_gasto', 'tipo_de_documento_ordenador_del_gasto', 'n_mero_de_documento_ordenador_del_gasto', 'nombre_supervisor', 'tipo_de_documento_supervisor', 'n_mero_de_documento_supervisor', 'nombre_ordenador_de_pago', 'tipo_de_documento_ordenador_de_pago', 'n_mero_de_documento_ordenador_de_pago', 'documentos_tipo', 'descripcion_documentos_tipo', 'fecha_de_inicio_del_contrato', 'ultima_actualizacion', 'fecha_de_notificaci_n_de_prorrogaci_n', 'fecha_inicio_liquidacion', 'fecha_fin_liquidacion']\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.DataFrame.from_records(results)\n",
    "print(\"Columnas cargadas:\")\n",
    "print(df_raw.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292887d6-c60b-46d3-907d-d8f4d25544c5",
   "metadata": {},
   "source": [
    "## **Cargar datos en Spark y explorar el esquema**\n",
    "\n",
    "Lo correcto es que Spark lea desde disco, no recrear DataFrames desde sí mismos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7a9c85-83d5-4163-8eb4-9c214e9e03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/07 19:08:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "26/02/07 19:08:49 WARN TaskSetManager: Stage 0 contains a task of very large size (86725 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros en Spark: 100,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/07 19:08:53 WARN TaskSetManager: Stage 3 contains a task of very large size (86725 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en: /opt/spark-data/raw/secop_contratos_general.parquet\n"
     ]
    }
   ],
   "source": [
    "# Pasar a Spark\n",
    "df_raw = spark.createDataFrame(df_raw)\n",
    "print(f\"Registros en Spark: {df_raw.count():,}\")\n",
    "\n",
    "# Guardar en formato parquet\n",
    "output_path = \"/opt/spark-data/raw/secop_contratos_general.parquet\"\n",
    "\n",
    "df_raw.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(f\"Datos guardados en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b5322-1db4-4b8d-90bd-a89396420624",
   "metadata": {},
   "source": [
    "## **Seleccionar columnas clave para ML**\n",
    "\n",
    "Campos clave: \n",
    " - Referencia del Contrato\n",
    " - Precio Base\n",
    " - Departamento\n",
    " - Tipo de Contrato\n",
    " - Fecha de Firma\n",
    " - Plazo de Ejecucion\n",
    " - Proveedor Adjudicado\n",
    " - Estado del Contrato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e908704e-6a1c-4000-b2c2-4b63b4228820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columnas seleccionadas para ML (8):\n",
      "- referencia_del_contrato\n",
      "- valor_del_contrato\n",
      "- departamento\n",
      "- tipo_de_contrato\n",
      "- fecha_de_firma\n",
      "- duraci_n_del_contrato\n",
      "- proveedor_adjudicado\n",
      "- estado_contrato\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/07 19:10:40 WARN TaskSetManager: Stage 4 contains a task of very large size (86725 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 4:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Registros para ML: 100,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columnas_clave_ml = [\n",
    "    \"referencia_del_contrato\",\n",
    "    \"valor_del_contrato\",\n",
    "    \"departamento\",\n",
    "    \"tipo_de_contrato\",\n",
    "    \"fecha_de_firma\",\n",
    "    \"duraci_n_del_contrato\",\n",
    "    \"proveedor_adjudicado\",\n",
    "    \"estado_contrato\"\n",
    "]\n",
    "\n",
    "# Validar columnas existentes\n",
    "columnas_disponibles = [c for c in columnas_clave_ml if c in df_raw.columns]\n",
    "\n",
    "print(f\"\\nColumnas seleccionadas para ML ({len(columnas_disponibles)}):\")\n",
    "for c in columnas_disponibles:\n",
    "    print(f\"- {c}\")\n",
    "\n",
    "df_ml = df_raw.select(*columnas_disponibles)\n",
    "\n",
    "print(f\"\\nRegistros para ML: {df_ml.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4ac82-1cc8-4313-b51b-ef084a7c41a0",
   "metadata": {},
   "source": [
    "## **Guardar en formato Parquet optimizado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238739c9-e29d-4b41-9814-fa5da3ec3739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando datos en formato Parquet...\n",
      "Ruta: /opt/spark-data/raw/secop_base_ml.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/07 19:10:54 WARN TaskSetManager: Stage 7 contains a task of very large size (86725 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 7:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados exitosamente en formato Parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path1 = \"/opt/spark-data/raw/secop_base_ml.parquet\"\n",
    "\n",
    "print(f\"Guardando datos en formato Parquet...\")\n",
    "print(f\"Ruta: {output_path1}\")\n",
    "\n",
    "(\n",
    "    df_ml.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(output_path1)\n",
    ")\n",
    "\n",
    "print(\"Datos guardados exitosamente en formato Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f86876-c840-4f5e-bf55-a77bc519cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
